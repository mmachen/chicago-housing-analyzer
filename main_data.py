import pandas as pd 
import argparse
import program_02_crime as crime 
import program_04_affordableHousing as afford
import time
import os
import json
import googlemaps
import commute as commute_utils  # canonical commute utilities
import datetime
from cache_util import Cache

# --- 1. COMMAND-LINE INTERFACE (CLI) SETUP ---
# Defines arguments that can be passed when running the script from the command line.
# This allows for flexible execution, like processing specific properties or skipping slow steps.
parser = argparse.ArgumentParser(description='Build housing dataset with optional sections and caching.')
parser.add_argument('--mls', type=str, default='', help='Comma-separated MLS IDs to process (others skipped).')
parser.add_argument('--mls-file', type=str, default='', help='Path to file with MLS IDs (one per line).')
parser.add_argument('--skip-commute', action='store_true', help='Skip commute time calculations.')
parser.add_argument('--skip-places', action='store_true', help='Skip nearby places/amenities calculations.')
parser.add_argument('--skip-crime', action='store_true', help='Skip crime score calculations.')
parser.add_argument('--skip-affordable', action='store_true', help='Skip affordable housing proximity calculations.')
parser.add_argument('--ttl-days', type=int, default=14, help='TTL for cache entries in days.')
parser.add_argument('--w-commute', type=float, default=0.4, help='Weight for commute score (0-1).')
parser.add_argument('--w-crime', type=float, default=0.3, help='Weight for crime safety (0-1).')
parser.add_argument('--w-amenities', type=float, default=0.2, help='Weight for amenities (0-1).')
parser.add_argument('--w-price', type=float, default=0.1, help='Weight for value (inverse price per sqft) (0-1).')

args, _ = parser.parse_known_args()

# --- 2. MLS ID FILTERING ---
# If --mls or --mls-file arguments are used, this section creates a 'set' of MLS IDs to process.
# A 'set' is used for very fast lookups inside the main loop.
# If the set is empty, all properties will be processed.
mls_include_set = set()
if args.mls:
    mls_include_set.update(x.strip() for x in args.mls.split(',') if x.strip())
if args.mls_file:
    try:
        with open(args.mls_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    mls_include_set.add(line)
    except Exception as e:
        print(f"Warning: failed to read MLS file {args.mls_file}: {e}")

# --- 3. DATA LOADING AND MERGING ---
# This section handles loading the raw property data and merging it with previously processed data.
# The goal is to update existing properties with new info from RedFin while preserving
# the expensive-to-calculate data (like commute times, crime scores) from the last run.

# Load the latest raw data from RedFin.
csv1_path = "data_sets/RedFin_raw_data.csv"
raw_df = pd.read_csv(csv1_path)
# Standardize column names for consistency.
raw_df.columns = ["SALE_TYPE","SOLD_DATE","PROPERTY_TYPE","ADDRESS","CITY","STATE","ZIP","PRICE","BEDS","BATHS","LOCATION","SQFT","LOT_SIZE","YEAR","DAY_ON_MARKET","PRICE_PER_SQFT","HOA","STATUS","NEXT_OPEN_S","NEXT_OPEN_E","URL","SOURCE","MLS","FAVORITE","INTERESTED","LATITUDE","LONGITUDE"]
# Ensure MLS ID is a string to prevent issues with scientific notation or floating point errors.
raw_df['MLS'] = raw_df['MLS'].astype(str)
# Remove rows that are missing essential location information.
raw_df = raw_df[~pd.isnull(raw_df["ADDRESS"])]
raw_df = raw_df[~pd.isnull(raw_df["CITY"])]

# Load the output file from the previous run, if it exists.
csv2_path = "output/final_data.csv"
try:
    # Load existing data, ensuring MLS is a string for correct merging.
    out_df = pd.read_csv(csv2_path, dtype={'MLS': str})

    # Identify columns that were generated by this script in the previous run.
    additional_columns = out_df.columns.difference(raw_df.columns).tolist()
    if additional_columns:
        # If there are generated columns, perform a 'right merge'.
        # This keeps all rows from the new `raw_df` and adds the old generated data
        # where the MLS ID matches.
        prop_df = pd.merge(out_df[additional_columns + ["MLS"]], raw_df, on="MLS", how="right")
    else:
        # If no extra columns, just use the raw data.
        prop_df = raw_df
except FileNotFoundError:
    # If no previous output file exists, we start fresh with only the raw data.
    print("No existing output file found. Starting fresh with raw data.")
    prop_df = raw_df
except Exception as e:
    # Handle other potential errors during file reading.
    print(f"Error reading output file: {e}")
    print("Starting fresh with raw data.")
    prop_df = raw_df

# Ensure the DataFrame has a clean, continuous index for looping with .at/.loc.
prop_df = prop_df.reset_index(drop=True)

# --- 4. DATA STANDARDIZATION & PREPARATION ---

# Function to clean up community area names for consistent merging.
# e.g., 'CHI - Loop' becomes 'loop'.
def standardize_location(location):
    if pd.isna(location):
        return location
    # Remove 'CHI - ' prefix if it exists and convert to lowercase
    location = str(location).lower()
    if location.startswith('chi - '):
        return location[6:]
    return location

# Standardize location names in the main DataFrame.
prop_df['LOCATION'] = prop_df['LOCATION'].apply(standardize_location)

# Load supplementary datasets and standardize their location names as well.
# Socioeconomic indicators (e.g., income, hardship index).
csvEco_path = "data_sets/socioeconomic_indicators.csv"
eco_df = pd.read_csv(csvEco_path)
eco_df['COMMUNITY AREA NAME'] = eco_df['COMMUNITY AREA NAME'].apply(standardize_location)

# Languages spoken per community area.
csvLanguage = "data_sets/languages_spoken.csv"
lang_df = pd.read_csv(csvLanguage)
lang_df['Community Area Name'] = lang_df['Community Area Name'].apply(standardize_location)
 
# --- 5. COLUMN PRE-ALLOCATION & VERSIONING ---
# This section defines all columns that will be generated by the script.
# Pre-defining them with correct data types (dtypes) is a best practice that:
#   1. Prevents pandas `PerformanceWarning` about DataFrame fragmentation.
#   2. Avoids dtype-related errors later in the script.

# By incrementing these version numbers, you can force a full refresh of commute
# or amenity data for all properties on the next run. This is useful when the
# underlying calculation logic changes.
COMMUTE_LOGIC_VERSION = 2 # Increment this to force-refresh all commute data

PLACES_LOGIC_VERSION = 2 # Increment this to force-refresh all places/amenity data

# A dictionary mapping every generated column to its intended data type.
ALL_GENERATED_COLUMNS = {
    # Commute columns
    'COMMUTE_TIME_work_Mike': 'object',
    'COMMUTE_STEPS_work_Mike': 'object',
    'COMMUTE_NUM_STEPS_work_Mike': 'float',
    'WALKING_TIME_work_Mike': 'object',
    'USES_BROWN_LINE_work_Mike': 'bool',
    'USES_RED_LINE_work_Mike': 'bool',
    'USES_BLUE_LINE_work_Mike': 'bool',    
    'USES_PINK_LINE_work_Mike': 'bool',
    'USES_GREEN_LINE_work_Mike': 'bool',
    'USES_ORANGE_LINE_work_Mike': 'bool',
    'USES_PURPLE_LINE_work_Mike': 'bool',
    'COMMUTE_VERSION_work_Mike': 'int',
    'COMMUTE_TIME_work_Xixi': 'object',
    'COMMUTE_STEPS_work_Xixi': 'object',
    'COMMUTE_NUM_STEPS_work_Xixi': 'float',
    'WALKING_TIME_work_Xixi': 'object',
    'USES_BROWN_LINE_work_Xixi': 'bool',
    'USES_RED_LINE_work_Xixi': 'bool',
    'USES_BLUE_LINE_work_Xixi': 'bool',    
    'USES_PINK_LINE_work_Xixi': 'bool',
    'USES_GREEN_LINE_work_Xixi': 'bool',
    'USES_ORANGE_LINE_work_Xixi': 'bool',
    'USES_PURPLE_LINE_work_Xixi': 'bool',
    'COMMUTE_VERSION_work_Xixi': 'int',
    'COMMUTE_TIME_school_Hana': 'object',
    'COMMUTE_STEPS_school_Hana': 'object',
    'COMMUTE_NUM_STEPS_school_Hana': 'float',
    'WALKING_TIME_school_Hana': 'object',
    'USES_BROWN_LINE_school_Hana': 'bool',
    'USES_RED_LINE_school_Hana': 'bool',
    'USES_BLUE_LINE_school_Hana': 'bool',    
    'USES_PINK_LINE_school_Hana': 'bool',
    'USES_GREEN_LINE_school_Hana': 'bool',
    'USES_ORANGE_LINE_school_Hana': 'bool',
    'USES_PURPLE_LINE_school_Hana': 'bool',
    'COMMUTE_VERSION_school_Hana': 'int',
    'DRIVE_TIME_school_Hana': 'object',
    # Amenity columns
    'GROCERY_CLOSEST': 'object', 'GROCERY_CLOSEST_DST': 'float', 'GROCERY_CLOSEST_WALK_TIME': 'object', 'GROCERY_WALK_NUM': 'float', 'GROCERY_LIST': 'object',
    'RESTAURANT_CLOSEST': 'object', 'RESTAURANT_CLOSEST_DST': 'float', 'RESTAURANT_CLOSEST_WALK_TIME': 'object', 'RESTAURANT_WALK_NUM': 'float', 'RESTAURANT_LIST': 'object',
    'WHOLE_FOODS_CLOSEST': 'object', 'WHOLE_FOODS_CLOSEST_DST': 'float', 'WHOLE_FOODS_CLOSEST_WALK_TIME': 'object', 'WHOLE_FOODS_WALK_NUM': 'float', 'WHOLE_FOODS_LIST': 'object',
    'TRADER_JOES_CLOSEST': 'object', 'TRADER_JOES_CLOSEST_DST': 'float', 'TRADER_JOES_CLOSEST_WALK_TIME': 'object', 'TRADER_JOES_WALK_NUM': 'float', 'TRADER_JOES_LIST': 'object',
    'LIQUOR_CLOSEST': 'object', 'LIQUOR_CLOSEST_DST': 'float', 'LIQUOR_CLOSEST_WALK_TIME': 'object', 'LIQUOR_WALK_NUM': 'float', 'LIQUOR_LIST': 'object',
    'BARS_CLOSEST': 'object', 'BARS_CLOSEST_DST': 'float', 'BARS_CLOSEST_WALK_TIME': 'object', 'BARS_WALK_NUM': 'float', 'BARS_LIST': 'object',
    'PARKS_CLOSEST': 'object', 'PARKS_CLOSEST_DST': 'float', 'PARKS_CLOSEST_WALK_TIME': 'object', 'PARKS_WALK_NUM': 'float', 'PARKS_LIST': 'object',
    'CHICAGO_PUBLIC_LIBRARY_CLOSEST': 'object', 'CHICAGO_PUBLIC_LIBRARY_CLOSEST_DST': 'float', 'CHICAGO_PUBLIC_LIBRARY_CLOSEST_WALK_TIME': 'object', 'CHICAGO_PUBLIC_LIBRARY_WALK_NUM': 'float', 'CHICAGO_PUBLIC_LIBRARY_LIST': 'object',
    # Other calculated columns
    'AFFORDABLE_NUM': 'float', 'AFFORDABLE_DESC': 'object',
    'GUN_SCORE': 'float', 'DRUG_SCORE': 'float', 'MURDER_SCORE': 'float', 'THEFT_SCORE': 'float', 'HUMAN_SCORE': 'float', 'OTHER_SCORE': 'float',
    'TOP_LANGUAGES': 'object'
}

# Loop through the defined columns and add them to the DataFrame if they don't exist.
# This ensures the DataFrame structure is consistent, regardless of whether we're
# starting fresh or loading an old file.
for col, dtype in ALL_GENERATED_COLUMNS.items():
    # Use pandas' nullable boolean type for boolean columns to handle missing values
    if dtype == 'bool':
        final_dtype = 'boolean'
    elif dtype == 'int':
        final_dtype = 'Int64' # Use nullable integer for version columns
    else:
        final_dtype = dtype

    if col not in prop_df.columns:
        prop_df[col] = pd.Series(dtype=final_dtype)
    else:
        # If the column exists (from CSV), ensure it has the correct dtype
        # to prevent FutureWarning when assigning a bool to a float column (full of NaNs).
        if str(prop_df[col].dtype) != str(final_dtype):
            prop_df[col] = prop_df[col].astype(final_dtype)

# --- 6. CONSTANTS AND INITIALIZATIONS ---

# Define fixed destinations for commute calculations.
COMMUTE_DESTINATIONS = {
    "work_Mike": "320 S Canal St, Chicago, IL",
    "work_Xixi": "100 N Carpenter St, Chicago, IL",
    "school_Hana": "4929 N Sawyer Ave, Chicago, IL"
}

# Define amenity types to search for. The key is our internal name,
# and the value is the term used in the Google Places API query.
AMENITY_TYPES = {
    "grocery": "grocery_or_supermarket",
    "restaurant": "restaurant",
    "liquor": "liquor_store",
    "bars": "bar",
    "parks": "park",
    # For specific brand names, 'keyword' search is used instead of 'type' in the API call.
    "chicago_public_library": "Chicago Public Library",
    "whole_foods": "Whole Foods",
    "trader_joes": "Trader Joe's"
}

# Initialize the Google Maps API client.
# The API key is loaded from a file that is NOT committed to version control.
try:
    with open("delete/input.txt", "r") as file:
        API_google = file.read().strip()
    gmaps = googlemaps.Client(key=API_google, queries_per_second=15)
except FileNotFoundError:
    print("Error: Google API key file not found at 'delete/input.txt'. Please create it.")
    exit()
except Exception as e:
    print(f"Error initializing Google Maps client: {e}")
    exit()

# Load and prepare the crime dataset.
# Dropping rows with missing data ensures calculations are valid.
csv2_path = "data_sets/Crimes_-_202103.csv"
crime_df = pd.read_csv(csv2_path)
crime_df.dropna(inplace=True)
crime_df.index = range(crime_df.shape[0])

# Load and prepare the affordable housing dataset.
csv_afford = "data_sets/Affordable_Rental_Housing_Developments.csv" 
afford_df1 = pd.read_csv(csv_afford) 
afford_df1 = afford_df1[["Address","Property Type","Latitude","Longitude"]]
afford_df1.columns = ["ADDRESS","DESCRIPTION","LATITUDE","LONGITUDE"] 
afford_df1.dropna(inplace=True)
affordable_df = afford_df1
# Dropping duplicates by location prevents over-counting.
affordable_df.drop_duplicates(subset = ["LATITUDE","LONGITUDE"],keep=False,inplace=True) 
affordable_df.index = range(affordable_df.shape[0])

# Get the total number of properties to process for progress tracking.
n = prop_df.shape[0]

# Initialize SQLite-based caches for API results.
# This saves money and time by storing API responses and reusing them on subsequent runs.
# The `ttl_days` argument automatically invalidates old data.
commute_cache = Cache(db_path='cache/cache.db', table='commute_cache', ttl_days=args.ttl_days)
places_cache = Cache(db_path='cache/cache.db', table='places_cache', ttl_days=args.ttl_days)

# --- Helper Functions for Data Parsing ---

def parse_distance_to_miles(distance_str):
    if isinstance(distance_str, (int, float)):
        return float(distance_str)
    if not isinstance(distance_str, str):
        return None
    
    value_str = distance_str.split()[0].replace(',', '')
    value = float(value_str)

    if 'mi' in distance_str:
        return value
    elif 'km' in distance_str:
        return value * 0.621371
    elif 'm' in distance_str:
        return value * 0.000621371
    return None # Ignore other units like 'ft' or time units

def parse_duration(duration_str):
    if not isinstance(duration_str, str):
        return None
    if 'min' in duration_str:
        return duration_str
    elif 'hour' in duration_str:
        hours = int(duration_str.split()[0])
        minutes = int(duration_str.split()[2]) if len(duration_str.split()) > 2 else 0
        return f"{hours * 60 + minutes} mins"
    else:
        try:
            return f"{int(duration_str.split()[0])} mins"
        except Exception:
            return None

# De-fragment the DataFrame after adding many columns to improve performance in the loop.
prop_df = prop_df.copy()

# Helper to determine if amenity data is missing for a property.
# This is used to decide whether to make new API calls.
def is_amenity_missing(row_idx: int, amenity_key_upper: str) -> bool:
    """Return True if key amenity fields are missing for the given row."""
    cols_to_check = [
        f"{amenity_key_upper}_CLOSEST",
        f"{amenity_key_upper}_CLOSEST_DST",
        f"{amenity_key_upper}_CLOSEST_WALK_TIME",
        f"{amenity_key_upper}_WALK_NUM",
        f"{amenity_key_upper}_LIST"
    ]
    for c in cols_to_check:
        if c not in prop_df.columns:
            return True
        val = prop_df.at[row_idx, c] if row_idx < len(prop_df) else pd.NA
        if pd.isna(val):
            return True
    return False

# --- 7. MAIN DATA PROCESSING LOOP ---
# This is the core of the script. It iterates over each property (row) in the DataFrame
# and calculates all the additional data points (commute, amenities, crime, etc.).
for i in range(0, n, 1):
    # If a specific set of MLS IDs was provided via CLI, skip any property not in that set.
    if mls_include_set:
        mls_val = str(prop_df.at[i, 'MLS']) if 'MLS' in prop_df.columns else ''
        if mls_val not in mls_include_set:
            continue
    
    # Print progress to the console.
    print("Percentage Complete: " + str(round((i+1)/n*100, 2)) + "%")
    print(f"Processing property {i+1} of {n}", flush=True)
    
    # Counter for API calls made for this single property.
    api_calls = 0
    
    # --- 7a. Commute Calculations ---
    home = prop_df['ADDRESS'][i] + " " + prop_df['CITY'][i]
    for dest_name, dest_address in COMMUTE_DESTINATIONS.items():
        # Define all column names for this specific destination.
        commute_time_col = f"COMMUTE_TIME_{dest_name}"
        commute_steps_col = f"COMMUTE_STEPS_{dest_name}"
        commute_num_steps_col = f"COMMUTE_NUM_STEPS_{dest_name}"
        walking_time_col = f"WALKING_TIME_{dest_name}"
        brown_line_col = f"USES_BROWN_LINE_{dest_name}"
        red_line_col = f"USES_RED_LINE_{dest_name}"
        blue_line_col = f"USES_BLUE_LINE_{dest_name}"
        pink_line_col = f"USES_PINK_LINE_{dest_name}"
        green_line_col = f"USES_GREEN_LINE_{dest_name}"
        orange_line_col = f"USES_ORANGE_LINE_{dest_name}"
        purple_line_col = f"USES_PURPLE_LINE_{dest_name}"
        version_col = f"COMMUTE_VERSION_{dest_name}"
        
        # Check if the commute data for this property is outdated or missing.
        # An update is needed if the stored version number is lower than the current logic version.
        current_version = prop_df.at[i, version_col]
        needs_update = pd.isna(current_version) or current_version < COMMUTE_LOGIC_VERSION

        # If not skipping and an update is needed, fetch new data.
        if (not args.skip_commute) and needs_update:
            # Create a unique key for caching based on version, origin, destination, and mode.
            cache_key = f"v{COMMUTE_LOGIC_VERSION}|{home}|{dest_name}|transit"
            # Try to get the result from the cache first.
            commute_info = commute_cache.get(cache_key)

            # Check if cached data is old (missing the new flags). If so, treat as a cache miss.
            if commute_info and 'USES_PINK_LINE' not in commute_info:
                print(f"  > Found old commute cache for {dest_name}. Refetching...")
                commute_info = None # Force refetch

            # If it's not in the cache (a "cache miss"), call the Google Maps API.
            if commute_info is None:
                print(f"  > Cache miss for commute to {dest_name}. Calling API...")
                commute_info = commute_utils.get_directions_from_google_api(
                    gmaps=gmaps,
                    origin=home,
                    location_address=dest_address,
                    mode='transit'
                )
                # Store the new result in the cache for next time.
                commute_cache.set(cache_key, commute_info)
                api_calls += 1
            
            prop_df.at[i, version_col] = COMMUTE_LOGIC_VERSION
            prop_df.at[i, commute_num_steps_col] = commute_info.get("COMMUTE_NUM_STEPS")
            prop_df.at[i, commute_time_col] = commute_info.get("COMMUTE_TIME")
            prop_df.at[i, commute_steps_col] = commute_info.get("COMMUTE_STEPS")
            prop_df.at[i, walking_time_col] = commute_info.get("WALKING_TIME")
            prop_df.at[i, brown_line_col] = commute_info.get("USES_BROWN_LINE", False)
            prop_df.at[i, red_line_col] = commute_info.get("USES_RED_LINE", False)
            prop_df.at[i, blue_line_col] = commute_info.get("USES_BLUE_LINE", False)
            prop_df.at[i, pink_line_col] = commute_info.get("USES_PINK_LINE", False)
            prop_df.at[i, green_line_col] = commute_info.get("USES_GREEN_LINE", False)
            prop_df.at[i, orange_line_col] = commute_info.get("USES_ORANGE_LINE", False)
            prop_df.at[i, purple_line_col] = commute_info.get("USES_PURPLE_LINE", False)
        
        # Special case: Get driving directions for the school destination.
        if dest_name == "school_Hana":
            drive_time_col = f"DRIVE_TIME_{dest_name}"
            if (not args.skip_commute) and pd.isnull(prop_df.at[i, drive_time_col]):
                cache_key = f"v{COMMUTE_LOGIC_VERSION}|{home}|{dest_name}|driving_5pm"
                drive_info = commute_cache.get(cache_key)
                if drive_info is None:
                    # Calculate next weekday at 5 PM (17:00) for peak traffic estimation
                    today = datetime.date.today()
                    # today.weekday() is 0 for Monday, 6 for Sunday
                    days_to_next_monday = (0 - today.weekday() + 7) % 7
                    
                    # If today is Monday and it's already past 5 PM, we want next week's Monday
                    if days_to_next_monday == 0 and datetime.datetime.now().time() > datetime.time(17, 0):
                        days_to_next_monday = 7
                        
                    target_date = today + datetime.timedelta(days=days_to_next_monday)
                    departure_for_drive = datetime.datetime.combine(target_date, datetime.time(17, 0))

                    drive_info = commute_utils.get_directions_from_google_api(
                        gmaps=gmaps,
                        origin=home,
                        location_address=dest_address,
                        mode='driving',
                        departure_time=departure_for_drive
                    )
                    commute_cache.set(cache_key, drive_info)
                    api_calls += 1
                prop_df.at[i, drive_time_col] = drive_info.get("COMMUTE_TIME")

    # --- 7b. Nearby Amenities Calculations ---
    homeLatLon = [prop_df['LATITUDE'][i], prop_df['LONGITUDE'][i]]
    
    for amenity_type, place_type in AMENITY_TYPES.items():
        # Define column names for this amenity.
        closest_col = f"{amenity_type.upper()}_CLOSEST"
        closest_dist_col = f"{amenity_type.upper()}_CLOSEST_DST"
        closest_walk_time_col = f"{amenity_type.upper()}_CLOSEST_WALK_TIME"
        walk_num_col = f"{amenity_type.upper()}_WALK_NUM"
        list_col = f"{amenity_type.upper()}_LIST"
        
        # Check if data is missing for this amenity. If so, fetch it.
        if (not args.skip_places) and is_amenity_missing(i, amenity_type.upper()):
            try:
                # Create a unique cache key based on version, location, and amenity type.
                cache_key = f"v{PLACES_LOGIC_VERSION}|{round(homeLatLon[0], 6)}|{round(homeLatLon[1], 6)}|{amenity_type}"
                places_data = places_cache.get(cache_key)
                
                # If cache is a miss or in an old format, call the API.
                if places_data is None or isinstance(places_data, list):
                    if isinstance(places_data, list):
                        print(f"  > Found old cache format for {amenity_type}. Refetching...")
                    else:
                        print(f"  > Cache miss for amenity '{amenity_type}'. Calling API...")
                    places_data = commute_utils.get_nearby_places(
                        gmaps=gmaps,
                        location=homeLatLon,
                        place_type=place_type
                    )
                    places_cache.set(cache_key, places_data)
                    api_calls += 2  # Places API can be 2+ calls (nearby search + distance matrix)
                    time.sleep(0.25)
                
                # Update the DataFrame with the results.
                if places_data:
                    prop_df.at[i, closest_col] = places_data.get('closest_name')
                    prop_df.at[i, closest_dist_col] = places_data.get('closest_distance_miles')
                    prop_df.at[i, closest_walk_time_col] = places_data.get('closest_walk_duration')
                    prop_df.at[i, walk_num_col] = places_data.get('count_within_half_mile', 0)
                    
                    nearby_list = places_data.get('nearby_places_list', [])
                    prop_df.at[i, list_col] = json.dumps(nearby_list) if nearby_list else None
                else: # Handle case where no places are found
                    prop_df.at[i, closest_col] = ""
                    prop_df.at[i, closest_dist_col] = None
                    prop_df.at[i, closest_walk_time_col] = None
                    prop_df.at[i, walk_num_col] = 0
                    prop_df.at[i, list_col] = None
            except Exception as e:
                print(f"Error processing {amenity_type}: {str(e)}")
                # Set default values for this amenity type
                prop_df.at[i, closest_col] = ""
                prop_df.at[i, closest_dist_col] = None
                prop_df.at[i, closest_walk_time_col] = None
                prop_df.at[i, walk_num_col] = 0
                prop_df.at[i, list_col] = None
    
    # Print total API calls for this property to monitor usage.
    print(f"Total API calls for property {i+1}: {api_calls}", flush=True)

    # --- 7c. Affordable Housing Calculation ---
    # This calculation is fast and doesn't use an API, so it's only done if the data is missing.
    if not args.skip_affordable and pd.isnull(prop_df.at[i, "AFFORDABLE_NUM"]):
        homeLatLon = [ prop_df['LATITUDE'][i] , prop_df['LONGITUDE'][i] ]
        zAfford = afford.get_afford_features(homeLatLon,affordable_df)
        prop_df.at[i,"AFFORDABLE_NUM"] = zAfford["NUM_AFFORDABLE_HOMES"] 
        prop_df.at[i,"AFFORDABLE_DESC"] = zAfford["AFFORDABLE_DESC"] 

    # --- 7d. Crime Score Calculation ---
    # Also a fast, local calculation. Only done if data is missing.
    if not args.skip_crime and pd.isnull(prop_df.at[i, "GUN_SCORE"]):
        homeLatLon = [ prop_df['LATITUDE'][i] , prop_df['LONGITUDE'][i] ]
        zCrime = crime.get_crime_features(homeLatLon,crime_df)
        prop_df.at[i, "GUN_SCORE"] = zCrime["GUN_SCORE"]
        prop_df.at[i, "DRUG_SCORE"] = zCrime["DRUG_SCORE"]
        prop_df.at[i, "MURDER_SCORE"] = zCrime["MURDER_SCORE"]
        prop_df.at[i, "THEFT_SCORE"] = zCrime["THEFT_SCORE"]
        prop_df.at[i, "HUMAN_SCORE"] = zCrime["HUMAN_SCORE"]
        prop_df.at[i, "OTHER_SCORE"] = zCrime["OTHER_SCORE"]

# --- 8. POST-LOOP DATA AGGREGATION AND SCORING ---

# Normalize raw crime scores into a 0-1 scale for easier interpretation and scoring.
# A score of 1 means the highest crime density in the dataset for that category.
crime_score_columns = ["GUN_SCORE", "DRUG_SCORE", "MURDER_SCORE", "THEFT_SCORE", "HUMAN_SCORE", "OTHER_SCORE"]
for col in crime_score_columns:
    if col not in prop_df.columns:
        print(f"Warning: Column {col} not found before normalization. Initializing with NA.")
        prop_df[col] = pd.NA

def _safe_minmax(series):
    s = series.astype(float)
    s_min = s.min()
    s_max = s.max()
    denom = (s_max - s_min)
    if pd.isna(s_min) or pd.isna(s_max) or denom == 0:
        return pd.Series([0] * len(s), index=s.index)
    return (s - s_min) / denom

prop_df["CRIME_GUN"] = _safe_minmax(prop_df["GUN_SCORE"]) 
prop_df["CRIME_DRUG"] = _safe_minmax(prop_df["DRUG_SCORE"]) 
prop_df["CRIME_MURDER"] = _safe_minmax(prop_df["MURDER_SCORE"]) 
prop_df["CRIME_THEFT"] = _safe_minmax(prop_df["THEFT_SCORE"]) 
prop_df["CRIME_HUMAN"] = _safe_minmax(prop_df["HUMAN_SCORE"]) 
prop_df["CRIME_OTHER"] = _safe_minmax(prop_df["OTHER_SCORE"]) 

# Merge the main property data with socioeconomic and language data based on the community area name.
prop_df = pd.merge(
    prop_df,
    eco_df,
    left_on="LOCATION",
    right_on="COMMUNITY AREA NAME",
    how="left",
    suffixes=('', '_eco')
)
prop_df.drop(columns=[col for col in prop_df if col.endswith('_eco')], inplace=True)

prop_df = pd.merge(
    prop_df,
    lang_df,
    left_on="LOCATION",
    right_on="Community Area Name",
    how="left",
    suffixes=('', '_lang')
)
prop_df.drop(columns=[col for col in prop_df if col.endswith('_lang')], inplace=True)

# Calculate and format the top 2 non-English languages for each property's community area.
def get_top_languages(row, language_cols):
    """Calculates and formats the top 2 languages for a given property row."""
    total_population = row.get('TOTAL')
    if pd.isna(total_population) or total_population <= 0:
        return ""

    lang_data = []
    for lang_col in language_cols:
        value = row.get(lang_col)
        if pd.notna(value):
            try:
                value = float(value)
                percentage = (value / total_population) * 100
                lang_data.append((lang_col, percentage))
            except (ValueError, TypeError):
                continue
    
    if not lang_data:
        return ""

    lang_data.sort(key=lambda x: x[1], reverse=True)
    top_langs = lang_data[:2]
    return ', '.join([f"{lang} ({pct:.1f}%)" for lang, pct in top_langs])

language_columns = [col for col in lang_df.columns if col not in ['Community Area Name', 'Community Area Number', 'TOTAL', 'ENGLISH', 'NON-ENGLISH']]
prop_df['TOP_LANGUAGES'] = prop_df.apply(lambda row: get_top_languages(row, language_columns), axis=1)

# Remove old, legacy columns that are no longer used to keep the final CSV clean.
cols_to_remove = [col for col in prop_df.columns if any(x in col for x in ["_DRIVE", "_SECOND_CLOSEST"])]
prop_df.drop(columns=cols_to_remove, inplace=True, errors='ignore') 

# --- 9. OVERALL SCORE CALCULATION ---
# This section combines multiple factors into a single "Overall Score" to rank properties.
# Each factor is normalized to a 0-1 scale (higher is better) and then combined using weights.

# Helper to normalize a series to a 0-1 range.
def _normalize(series):
    s = series.astype(float)
    s_min = s.min()
    s_max = s.max()
    denom = (s_max - s_min)
    if pd.isna(s_min) or pd.isna(s_max) or denom == 0:
        return pd.Series([0.5] * len(s), index=s.index)
    return (s - s_min) / denom
    
# Calculates a commute score. Lower total commute time results in a higher score.
def _calculate_commute_score(df: pd.DataFrame) -> pd.Series:
    """Calculates a normalized commute score. Lower commute time is better."""
    def _extract_minutes(text):
        try:
            if isinstance(text, str) and 'min' in text:
                return float(text.split()[0])
        except Exception:
            return None
        return None

    mike_commute = df['COMMUTE_TIME_work_Mike'].apply(_extract_minutes).fillna(0)
    xixi_commute = df['COMMUTE_TIME_work_Xixi'].apply(_extract_minutes).fillna(0)
    total_commute_minutes = mike_commute + xixi_commute
    
    # Invert the score so higher is better (1 - normalized time)
    return 1 - _normalize(total_commute_minutes.replace(0, pd.NA))

# Calculates a safety score. Lower combined crime risk results in a higher score.
def _calculate_crime_safety_score(df: pd.DataFrame) -> pd.Series:
    """Calculates a normalized crime safety score. Lower crime risk is better."""
    crime_risk = (_safe_minmax(df['GUN_SCORE'].fillna(0)) +
                  _safe_minmax(df['DRUG_SCORE'].fillna(0)) +
                  _safe_minmax(df['MURDER_SCORE'].fillna(0)) +
                  _safe_minmax(df['THEFT_SCORE'].fillna(0)) +
                  _safe_minmax(df['HUMAN_SCORE'].fillna(0)) +
                  _safe_minmax(df['OTHER_SCORE'].fillna(0))) / 6.0
    return 1 - crime_risk

# Calculates an amenities score. More nearby amenities results in a higher score.
def _calculate_amenities_score(df: pd.DataFrame) -> pd.Series:
    """Calculates a normalized amenities score based on number of nearby places."""
    total_amenities = (df['GROCERY_WALK_NUM'].fillna(0) +
                       df['RESTAURANT_WALK_NUM'].fillna(0) +
                       df['PARKS_WALK_NUM'].fillna(0) +
                       df['CHICAGO_PUBLIC_LIBRARY_WALK_NUM'].fillna(0))
    return _normalize(total_amenities)

# Calculates a value score. A lower price per square foot results in a higher score.
def _calculate_value_score(df: pd.DataFrame) -> pd.Series:
    """Calculates a normalized value score based on price per square foot."""
    # Calculate inverse price per sqft (higher is better value)
    price_per_sqft = df['PRICE'].replace(0, pd.NA) / df['SQFT'].replace(0, pd.NA)
    value_metric = 1 / price_per_sqft
    return _normalize(value_metric)

# Calculate each individual score component.
commute_score = _calculate_commute_score(prop_df)
crime_safety_score = _calculate_crime_safety_score(prop_df)
amenities_score = _calculate_amenities_score(prop_df)
value_score = _calculate_value_score(prop_df)

# Combine the individual scores using the weights provided via CLI arguments.
# .fillna() is used to handle cases where a score could not be calculated, providing a neutral value.
prop_df['OVERALL_SCORE'] = (
    args.w_commute * commute_score.fillna(0.5) +
    args.w_crime * crime_safety_score.fillna(0.5) +
    args.w_amenities * amenities_score.fillna(0.0) +
    args.w_price * value_score.fillna(0.0)
)

# --- 10. SAVE FINAL DATASET ---
# Write the fully processed and scored DataFrame to a CSV file.
# This file is what the Flask web application reads.
output_file_path = "output/final_data.csv"
prop_df.to_csv(output_file_path, index=False)
print("Data update complete!")
